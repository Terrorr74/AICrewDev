# AICrewDev - Advanced AI Development Team Simulator

[![Python 3.11+](https://img.shields.io/badge/python-3.11+-blue.svg)](https://www.python.org/downloads/)
[![CrewAI](https://img.shields.io/badge/CrewAI-Latest-green.svg)](https://github.com/joaomdmoura/crewAI)
[![Docker](https://img.shields.io/badge/Docker-Required-blue.svg)](https://www.docker.com/)
[![Async](https://img.shields.io/badge/Async-Enabled-purple.svg)]()
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

AICrewDev is a sophisticated AI-powered development team simulator built on the CrewAI framework. It creates specialized AI agents that work together as a cohesive development team to tackle complex software development projects.

## ğŸŒŸ Key Features

### âš¡ **Asynchronous Operations** (NEW!)
- **Concurrent Agent Creation**: Create multiple agents simultaneously for faster team building
- **Parallel Task Execution**: Execute development tasks concurrently for improved performance
- **Real-time Progress Monitoring**: Track operations as they happen with live updates
- **Background Processing**: Long-running operations with non-blocking execution

### ğŸ›¡ï¸ **Production-Ready Validation** (NEW!)
- **Comprehensive Configuration Validation**: Prevent errors before deployment
- **System Health Checks**: Validate dependencies and requirements
- **Docker Integration Validation**: Ensure safe code execution environment
- **LLM Provider Validation**: Verify API connectivity and model availability

### ğŸ³ **Docker Integration** (NEW!)
- **Safe Code Execution**: All code runs in isolated Docker containers
- **Automatic Container Management**: Containers are created, managed, and cleaned up automatically
- **Multi-Environment Support**: Support for different development environments
- **Security-First Approach**: Isolated execution prevents system compromise

### ğŸ¤– **Multi-Agent Architecture**
- **Specialized Agent Roles**: Tech Lead, Developer, Code Reviewer, Project Manager
- **Role-Optimized LLM Settings**: Each agent type uses optimized temperature and parameters
- **Collaborative Workflows**: Agents work together on complex development tasks
- **Scalable Team Sizes**: Support for minimal, standard, and large team configurations

### ğŸ”— **Multi-Provider LLM Support**
- **OpenAI**: GPT-4, GPT-3.5-turbo with native CrewAI integration
- **Anthropic**: Claude-3 models with enhanced reasoning capabilities
- **Ollama**: Local LLM deployment for privacy and cost control
- **Groq**: High-speed inference for rapid development cycles

### ğŸ“Š **Advanced Monitoring & Observability**
- **Real-time Metrics**: CPU, memory, and operation tracking
- **Performance Analytics**: Response times, success rates, and throughput
- **Health Monitoring**: System health checks and dependency validation
- **Structured Logging**: Comprehensive logging for debugging and analysis

## ğŸš€ Quick Start

### Prerequisites

- **Python 3.11+** - Required for modern async features
- **Docker Desktop** - Essential for safe code execution
- **Git** - For version control and repository management

### Installation

```bash
# Clone the repository
git clone <repository-url>
cd AICrewDev

# Install dependencies
pip install -r requirements.txt

# Install in development mode
pip install -e .

# Verify Docker installation
docker --version
docker run hello-world
```

### Basic Usage

```python
import asyncio
from src.config.llm_config import LLMConfig, LLMProvider
from src.agents.async_agents import run_development_workflow_async

async def main():
    # Configure your LLM provider
    config = LLMConfig(
        provider=LLMProvider.OPENAI,
        model_name="gpt-4o-mini",
        temperature=0.7
    )
    
    # Run an async development workflow
    result = await run_development_workflow_async(
        project_type="web_application",
        llm_config=config,
        team_size="standard"
    )
    
    if result.status == "completed":
        print("ğŸ‰ Development workflow completed!")
        print(f"Result: {result.result}")
    else:
        print(f"âŒ Workflow failed: {result.error}")

# Run the async workflow
asyncio.run(main())
```

## ğŸ—ï¸ Architecture Overview

AICrewDev uses a layered architecture designed for scalability and maintainability:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Application Layer                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Async Agents    â”‚  Validation    â”‚  Monitoring    â”‚ Docker â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚              Core Services Layer                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Agent Factory   â”‚  Crew Manager  â”‚  Task Factory          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                  Configuration Layer                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  LLM Config      â”‚  Agent Config  â”‚  System Config         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                   Foundation Layer                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚              CrewAI Framework & LLM Providers               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Core Components

- **Async Agent Factory**: Creates and manages agents concurrently
- **Crew Manager**: Orchestrates team workflows and task execution
- **Configuration Validation**: Ensures all settings are correct before deployment
- **Docker Integration**: Provides safe, isolated code execution environment
- **Real-time Monitoring**: Tracks performance and system health
- **Multi-Provider LLM Support**: Integrates with various AI model providers

## ğŸ“š Documentation

### Quick Links
- ğŸ“– **[Advanced Guide](docs/ADVANCED_GUIDE.md)** - Comprehensive documentation with examples
- ğŸ **[Getting Started](docs/getting_started.md)** - Step-by-step setup guide
- ğŸ›ï¸ **[Architecture](docs/architecture.md)** - Detailed system architecture
- âš™ï¸ **[LLM Configuration](docs/llm_configuration.md)** - Provider-specific setup

### Example Scripts
- ğŸš€ **[Async Agents Demo](examples/async_agents_demo.py)** - Showcase async capabilities
- ğŸ”§ **[Enhanced Architecture](examples/enhanced_architecture_demo.py)** - Full system demo
- ğŸ“Š **[Real-time Monitoring](examples/enhanced_real_time_demo.py)** - Monitoring features
- ğŸ¦™ **[Ollama Integration](examples/ollama_llama2_demo.py)** - Local LLM usage

## âš¡ Async Capabilities (NEW!)

### Concurrent Agent Creation

Create multiple agents simultaneously for faster team building:

```python
from src.agents.async_agents import AsyncAgentFactory

async def create_team():
    factory = AsyncAgentFactory(max_workers=4)
    
    agent_configs = [
        {"role": "frontend_developer", "goal": "Build React UIs", ...},
        {"role": "backend_developer", "goal": "Create APIs", ...},
        {"role": "devops_engineer", "goal": "Manage deployments", ...}
    ]
    
    # Create all agents concurrently
    results = await factory.create_agents_batch_async(agent_configs, config)
    agents = [r.result for r in results if r.status == "completed"]
    
    return agents
```

### Parallel Task Execution

Execute development tasks concurrently:

```python
# Execute tasks in parallel for better performance
result = await factory.execute_tasks_async(
    tasks=development_tasks,
    agents=team_agents,
    execution_mode="parallel"  # or "sequential"
)
```

### Real-time Operation Monitoring

Monitor long-running operations:

```python
# Start operation and monitor progress
operation = await factory.create_development_team_async(config)

# Track progress in real-time
while not operation.is_complete:
    print(f"Status: {operation.status}, Duration: {operation.duration}s")
    await asyncio.sleep(1)
```

## ğŸ›¡ï¸ Configuration Validation (NEW!)

Comprehensive validation prevents configuration errors:

```python
from src.config.validators import (
    AgentConfigValidator,
    LLMConfigValidator,
    SystemConfigValidator
)

# Validate agent configuration
agent_config = {
    "role": "developer",
    "goal": "Write clean code",
    "backstory": "Experienced developer",
    "allow_code_execution": True
}

validator = AgentConfigValidator(**agent_config)
print("âœ… Agent configuration is valid")

# Validate system requirements
system_validator = SystemConfigValidator()
validation_result = system_validator.validate_system()

if validation_result.is_valid:
    print("âœ… System ready for AICrewDev")
else:
    for issue in validation_result.issues:
        print(f"âŒ {issue}")
```

## ğŸ³ Docker Integration (NEW!)

Safe code execution in isolated containers:

```python
# Agents automatically use Docker for code execution
agent = Agent(
    role="developer",
    allow_code_execution=True,  # Uses Docker automatically
    goal="Write and test code safely"
)

# Docker containers are managed automatically:
# - Created when needed
# - Isolated from host system
# - Cleaned up after use
# - Security-first approach
```

## ğŸ“Š Monitoring & Observability

Real-time monitoring and metrics collection:

```python
from src.monitoring.real_time_monitor import RealTimeMonitor

# Start monitoring
monitor = RealTimeMonitor()
await monitor.start()

# Get real-time metrics
metrics = monitor.get_current_metrics()
print(f"Active operations: {metrics.active_operations}")
print(f"Memory usage: {metrics.memory_usage_mb}MB")
print(f"CPU usage: {metrics.cpu_usage_percent}%")
```

## ğŸ”§ Configuration

### Environment Variables

Create a `.env` file:

```bash
# LLM Provider API Keys
OPENAI_API_KEY=your_openai_api_key
ANTHROPIC_API_KEY=your_anthropic_api_key
GROQ_API_KEY=your_groq_api_key

# Ollama Configuration (if not running locally)
OLLAMA_BASE_URL=http://localhost:11434

# Docker Settings
DOCKER_TIMEOUT=300
DOCKER_MEMORY_LIMIT=2g

# Logging
LOG_LEVEL=INFO
LOG_FILE=logs/aicrewdev.log
```

### LLM Provider Setup

#### OpenAI
```python
config = LLMConfig(
    provider=LLMProvider.OPENAI,
    model_name="gpt-4o-mini",
    temperature=0.7,
    api_key="your-api-key"  # Or use OPENAI_API_KEY env var
)
```

#### Ollama (Local)
```bash
# Install and start Ollama
ollama pull llama2
ollama serve

# Configure AICrewDev
config = LLMConfig(
    provider=LLMProvider.OLLAMA,
    model_name="llama2",
    base_url="http://localhost:11434"
)
```

#### Anthropic
```python
config = LLMConfig(
    provider=LLMProvider.ANTHROPIC,
    model_name="claude-3-haiku-20240307",
    api_key="your-api-key"  # Or use ANTHROPIC_API_KEY env var
)
```

## ğŸ¯ Use Cases

### Web Development Team
```python
# Create a specialized web development team
result = await factory.create_development_team_async(
    config=config,
    project_type="web_application",
    team_size="standard"
)
```

### Mobile App Development
```python
# Mobile-focused development workflow
result = await run_development_workflow_async(
    project_type="mobile_app",
    llm_config=config,
    custom_tasks=[
        "Design mobile app architecture",
        "Implement native iOS/Android features",
        "Create responsive UI components",
        "Set up app store deployment"
    ]
)
```

### AI/ML Project Team
```python
# AI project with specialized agents
agent_configs = [
    {
        "role": "data_scientist",
        "goal": "Build machine learning models",
        "backstory": "ML expert with Python and TensorFlow"
    },
    {
        "role": "ml_engineer",
        "goal": "Deploy models to production",
        "backstory": "MLOps specialist with cloud platforms"
    }
]
```

## ğŸ§ª Testing

Run the test suite:

```bash
# Run all tests
python -m pytest tests/

# Run specific test categories
python -m pytest tests/test_enhanced_config.py
python -m pytest tests/test_llm.py
python -m pytest tests/test_main.py

# Run with verbose output
python -m pytest -v tests/
```

## ğŸ¤ Contributing

We welcome contributions! Please see our contributing guidelines:

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

### Development Setup

```bash
# Clone your fork
git clone https://github.com/your-username/aicrewdev.git
cd aicrewdev

# Install development dependencies
pip install -r requirements.txt
pip install -e .

# Install pre-commit hooks
pre-commit install

# Run tests before committing
python -m pytest tests/
```

## ğŸ“‹ Roadmap

### Phase 2 (Current)
- âœ… Asynchronous agent operations
- âœ… Configuration validation system
- âœ… Docker integration for safe execution
- âœ… Enhanced documentation

### Phase 3 (Next)
- ğŸ”„ Web-based management interface
- ğŸ”„ Integration with popular IDEs
- ğŸ”„ Advanced workflow templates
- ğŸ”„ Plugin system for extensibility

### Phase 4 (Future)
- ğŸ”„ Distributed agent execution
- ğŸ”„ Advanced AI model fine-tuning
- ğŸ”„ Enterprise security features
- ğŸ”„ Cloud deployment options

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ†˜ Support

- ğŸ“– **Documentation**: [Advanced Guide](docs/ADVANCED_GUIDE.md)
- ğŸ› **Bug Reports**: [GitHub Issues](https://github.com/your-repo/aicrewdev/issues)
- ğŸ’¬ **Discussions**: [GitHub Discussions](https://github.com/your-repo/aicrewdev/discussions)
- ğŸ“§ **Email**: support@aicrewdev.com

## ğŸ™ Acknowledgments

- **CrewAI Team** - For the excellent multi-agent framework
- **OpenAI, Anthropic, Ollama** - For providing powerful LLM capabilities
- **Docker** - For containerization and safe execution
- **Python Community** - For the amazing ecosystem and tools

---

**Made with â¤ï¸ by the AICrewDev Team**

*Empowering developers with AI-powered development teams*
